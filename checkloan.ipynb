{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"/home/xavient/loan3\")\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xavient/.local/lib/python3.5/site-packages/ipykernel/__main__.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#machine learning libraries\n",
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iter_no = 5\n",
    "gp_params = {'alpha': 1e-5}\n",
    "cv_splits = 8\n",
    "\n",
    "\n",
    "def treesCV(eta, gamma,max_depth,min_child_weight,subsample,colsample_bytree,n_estimators):\n",
    "    #function for cross validation gradient boosted trees\n",
    "    return cross_val_score(xgb.XGBRegressor(objective='binary:logistic',\n",
    "                                                tree_method = 'hist',\n",
    "                                                learning_rate=max(eta,0),\n",
    "                                                gamma=max(gamma,0),\n",
    "                                                max_depth=int(max_depth),\n",
    "                                                min_child_weight=int(min_child_weight),\n",
    "                                                silent=True,\n",
    "                                                subsample=max(min(subsample,1),0.0001),\n",
    "                                                colsample_bytree=max(min(colsample_bytree,1),0.0001),\n",
    "                                                n_estimators=int(n_estimators),\n",
    "                                                seed=42,nthread=-1), X=X_train, y=y_train, cv=cv_splits, n_jobs=-1).mean()\n",
    "\n",
    "def recode_target(rating):\n",
    "    \n",
    "    # Recode the target variable as described in scope\n",
    "    if rating == 'Y':\n",
    "        return 1\n",
    "    elif rating == 'N':\n",
    "        return 0\n",
    "    else:\n",
    "        return \"missing\"\n",
    "\n",
    "def data_prep_train(data_df):\n",
    "\n",
    "    #how to handle types\n",
    "    data_df = data_df.set_index('Loan_ID')\n",
    "    data_df_num = data_df.select_dtypes(exclude=object)\n",
    "    data_df_obj = data_df.select_dtypes(include=object)\n",
    "    data_df_obj['Loan_Status'] = data_df_obj['Loan_Status'].apply(lambda x: recode_target(x))\n",
    "\n",
    "    #how to handle nan\n",
    "    data_df_num = data_df_num.fillna(data_df_num.mean())\n",
    "\n",
    "    #get dummy variables\n",
    "    data_df_obj = pd.get_dummies(data_df_obj, dummy_na=True)\n",
    "    \n",
    "    return pd.concat([data_df_num, data_df_obj],axis=1)\n",
    "\n",
    "\n",
    "def data_prep_test(data_df):\n",
    "\n",
    "    #how to handle types\n",
    "    data_df = data_df.set_index('Loan_ID')\n",
    "    data_df_num = data_df.select_dtypes(exclude=object)\n",
    "    data_df_obj = data_df.select_dtypes(include=object)\n",
    "    \n",
    "    #how to handle nan\n",
    "    data_df_num = data_df_num.fillna(data_df_num.mean())\n",
    "\n",
    "    #get dummy variables\n",
    "    data_df_obj = pd.get_dummies(data_df_obj, dummy_na=True)\n",
    "    \n",
    "    return pd.concat([data_df_num, data_df_obj],axis=1)\n",
    "\n",
    "# reading data\n",
    "data_train = pd.read_csv('train.csv', sep=',')\n",
    "data_train = data_prep_train(data_train)\n",
    "\n",
    "data_pred = pd.read_csv('test.csv', sep=',')\n",
    "data_pred = data_prep_test(data_pred)\n",
    "    \n",
    "\n",
    "#train test split doesnt actually split\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(data_train.drop(['Loan_Status'],axis=1)),\\\n",
    "                                                        np.array(data_train['Loan_Status']), test_size=0, random_state=42)\n",
    "X_test = np.array(data_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m--------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |       eta |     gamma |   max_depth |   min_child_weight |   n_estimators |   subsample | \n"
     ]
    }
   ],
   "source": [
    "#Bayesian Hyper parameter optimization of gradient boosted trees\n",
    "treesBO = BayesianOptimization(treesCV,{'eta':(0.001,0.4),\n",
    "                                        'gamma':(8,12),\n",
    "                                        'max_depth':(400,700),\n",
    "                                        'min_child_weight':(0.1,1),\n",
    "                                        'subsample':(0.3,0.6),\n",
    "                                        'colsample_bytree':(0.6,1),\n",
    "                                        'n_estimators':(600,800)})\n",
    "treesBO.maximize(n_iter=iter_no, **gp_params)\n",
    "tree_best = treesBO.res['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train tree with best paras\n",
    "trees_model = xgb.XGBRegressor(objective='binary:logistic',\n",
    "                                tree_method = 'hist',\n",
    "                                seed=42,\n",
    "                                learning_rate=max(tree_best['max_params']['eta'],0),\n",
    "                                gamma=max(tree_best['max_params']['gamma'],0),\n",
    "                                max_depth=int(tree_best['max_params']['max_depth']),\n",
    "                                min_child_weight=int(tree_best['max_params']['min_child_weight']),\n",
    "                                silent=True,\n",
    "                                subsample=max(min(tree_best['max_params']['subsample'],1),0.0001),\n",
    "                                colsample_bytree=max(min(tree_best['max_params']['colsample_bytree'],1),0.0001),\n",
    "                                n_estimators=int(tree_best['max_params']['n_estimators']),nthread=-1)\n",
    "trees_model.fit(X_train, y_train)\n",
    "y_hat = trees_model.predict(np.array(X_test))\n",
    "\n",
    "\n",
    "submission = pd.DataFrame({'Loan_ID':np.array(data_pred['Loan_ID']),'Loan_Status':y_hat})\n",
    "\n",
    "#write to file for submission\n",
    "submission.to_csv('submission.csv',sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
